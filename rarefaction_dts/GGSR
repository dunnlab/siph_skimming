"""
granular geneome size rarefaction

This snakefile's main goal is to create a fast, scalable rarefaction analysis
for k-mer based genome size estimation.


Works by breaking the fastq files into 100 pieces, 
author: dts
github: @conchoecia
date: dec 2022
license: none atm
"""

import gzip
from Bio import SeqIO
import pandas as pd

configfile: "config.yaml"

config["tool"] = "GGSR_analysis"
config["ks"] = [21]
config["num_splits"] = 100


# turn the data structure into something easier to work with later. we can access individual indices
config["reads"] = {}
for sample in config["sample"]:
    config["reads"][sample] = {i:config["sample"][sample]["reads"][i] for i in range(len(config["sample"][sample]["reads"]))}

# now define which reads we want, since different samples will have different numbers of reads
reads_output = []
for sample in config["sample"]:
    for i in range(len(config["sample"][sample]["reads"])):
        for fn in range(0,config["num_splits"]):
            reads_output.append(config["tool"] + "/fastqs/{sample}/{sample}.{i}.{num}.fastq.gz".format(
                sample=sample, i=i, num=fn ))

rule all:
    input:
        reads_output,
        #expand(config["tool"] + "/jellyfish/{sample}/{sample}.{k}.{num}.jf",
        #    sample=config["sample"], num=list(range(0,config["num_splits"])), k = config["ks"]),
        #expand(config["tool"] + "/histos/{sample}/{sample}.{k}.{num}.histo",
        #    sample=config["sample"], num=list(range(0,config["num_splits"])), k = config["ks"]),
        expand(config["tool"] + "/genomescope/{sample}/{sample}.{k}.{num}_linear_plot.png",
            sample=config["sample"],
            num=list(range(0,config["num_splits"])),
            k = config["ks"]),
        expand(config["tool"] + "/gifs/{sample}.{k}.gif",
            sample=config["sample"],
            k = config["ks"]),
        expand(config["tool"] + "/summary_tables/{sample}.{k}.summary_table.tsv",
            sample=config["sample"],
            k = config["ks"]),
        expand(config["tool"] + "/genomesize_plots/{sample}.{k}.genomesize.pdf",
            sample=config["sample"],
            k = config["ks"])

rule randomize_and_split_one_fastq_per_sample:
    """
    This rule splits the files into 100 pieces each.
     This was 120 times faster than splitting them with python.
    """
    input:
        fastq = lambda wildcards: config["reads"][wildcards.sample][int(wildcards.index)]
    output:
        fastqs = expand(config["tool"] + "/fastqs/{{sample}}/{{sample}}.{{index}}.{num}.fastq.gz",
                        num=list(range(0,config["num_splits"])))
    params:
        outstring = lambda wildcards: " -o ".join([ config["tool"] + "/fastqs/{sample}/{sample}.{index}.{num}.fastq.gz".format(
                        sample = wildcards.sample, index = wildcards.index, num=i) for i in range(0,config["num_splits"])])
    threads: 1
    shell:
        """
        # make a system call to fastqsplitter to split the file into 100 pieces
        fastqsplitter -i {input.fastq} -o {params.outstring}
        """

rule run_jellyfish_on_each_fastq_file:
    """
    Runs jellyfish on each of the 100 fastq files for every sample.
      The output file for this will be a database instead of a histo file.
    """
    input:
        fastqs = lambda wildcards: expand(config["tool"] + "/fastqs/{{sample}}/{{sample}}.{index}.{{num}}.fastq.gz",
                        index=range(0,len(config["sample"][wildcards.sample]["reads"])))
    output:
        jellyfishdb = config["tool"] + "/jellyfish/{sample}/{sample}.{k}.{num}.jf"
    params:
        k = lambda wildcards: wildcards.k
    threads: 1
    shell:
        """
        zcat {input.fastqs} | \
          jellyfish count -C -m {params.k} -s 1000000000 -t {threads} /dev/fd/0 -o {output.jellyfishdb}
        """

rule generate_new_jellyfish_file_ascending_sum:
    """
    make new jellyfish files by summing the kmers from the previous jellyfish file.

    This rule also makes the histogram file, and we immediately delete the temporary .jf file.
        The temporary .jf files will linger if we don't do them successively and will blow up the disk usage.
    """
    input:
        jellyfishdb = lambda wildcards: expand(config["tool"] + "/jellyfish/{{sample}}/{{sample}}.{{k}}.{num}.jf",
                             num = range(0,int(wildcards.num)+1))
    output:
        final_jf = temp(config["tool"] + "/jellyfish/{sample}/tempmerged/{sample}.{k}.{num}.jf"),
        histo = config["tool"] + "/histos/{sample}/{sample}.{k}.{num}.histo"
    threads: 1
    params:
        thisnum = lambda wildcards: wildcards.num
    shell:
        """
        if [[ {params.thisnum} -eq 0 ]]
        then
            cp {input.jellyfishdb} {output.final_jf}
        else
            jellyfish merge -o {output.final_jf} {input.jellyfishdb}
        fi

        jellyfish histo -t {threads} {output.final_jf} > {output.histo}
        """

rule run_genomescope_on_each_histo:
    input:
        histo   = config["tool"] + "/histos/{sample}/{sample}.{k}.{num}.histo"
    output:
        plot    = config["tool"] + "/genomescope/{sample}/{sample}.{k}.{num}_linear_plot.png",
        summary = config["tool"] + "/genomescope/{sample}/{sample}.{k}.{num}_summary.txt"
    params:
        outdir = lambda wildcards: "{tool}/genomescope/{sample}/".format(
                                    tool=config["tool"],
                                    sample=wildcards.sample,
                                    ),
        outprefix = lambda wildcards: "{sample}.{k}.{num}".format(
                                       sample=wildcards.sample,
                                       k=wildcards.k,
                                       num=wildcards.num
                                       ),
        sample = lambda wildcards: wildcards.sample,
        k = lambda wildcards: wildcards.k,
    threads: 1
    shell:
        """
        genomescope.R -i {input.histo} -o {params.outdir} \
          -k {params.k} -n {params.outprefix}
        """

rule make_an_animation_for_each_sample:
    """
    just makes an animation for each sample. Shows how the kmer spectrum changes with data amount
    """
    input:
        pngs = expand(config["tool"] + "/genomescope/{{sample}}/{{sample}}.{{k}}.{num}_linear_plot.png",
            num=list(range(0,config["num_splits"])))
    output:
        gif = config["tool"] + "/gifs/{sample}.{k}.gif"
    params:
        pngs = lambda wildcards: ",".join([config["tool"] + "/genomescope/{sample}/{sample}.{k}.{num}_linear_plot.png".format(
            sample = wildcards.sample, k = wildcards.k, num=i) for i in range(0,config["num_splits"])]),
        prefix = lambda wildcards: config["tool"] + "/genomescope/{sample}/{sample}.{k}.".format(
            sample = wildcards.sample, k = wildcards.k)
    threads: 1
    shell:
        """
        ffmpeg -f image2 -i {params.prefix}%d_linear_plot.png -vf scale=512:-1 {output.gif}
        """

rule make_a_summary_table:
    """
    Make a summary table of the genomescope results for each num and k

    The contents of each input file look like this, and we need to parse it:
    GenomeScope version 2.0
    input file = GGSR_analysis/histos/Hcal/Hcal.21.57.histo
    output directory = GGSR_analysis/genomescope/Hcal/
    p = 2
    k = 21
    name prefix = Hcal.21.57
    
    property                      min               max
    Homozygous (aa)               96.573%           96.9026%
    Heterozygous (ab)             3.0974%           3.42695%
    Genome Haploid Length         100,974,888 bp    103,012,576 bp
    Genome Repeat Length          34,098,896 bp     34,787,017 bp
    Genome Unique Length          66,875,991 bp     68,225,559 bp
    Model Fit                     72.2123%          97.2243%
    Read Error Rate               0.217933%         0.217933%
    """
    input:
        summaries = expand(config["tool"] + "/genomescope/{{sample}}/{{sample}}.{{k}}.{num}_summary.txt",
            num=list(range(0,config["num_splits"])))
    output:
        summary_table = config["tool"] + "/summary_tables/{sample}.{k}.summary_table.tsv"
    threads: 1
    run:
        entries = []

        list_of_results = []
        for thisnum in range(0,config["num_splits"]):               
            targetfile = config["tool"] + "/genomescope/{sample}/{sample}.{k}.{num}_summary.txt".format(
                sample = wildcards.sample, k = wildcards.k, num=thisnum)
            with open(targetfile, "r") as f:
                dict_of_vals = {}
                dict_of_vals["sample"] = wildcards.sample 
                dict_of_vals["fraction_of_total_data"] = (thisnum + 1) / config["num_splits"]
                dict_of_vals["k"] = wildcards.k
                start_count = False
                counter = 0
                for line in f:
                    line = line.strip()
                    if line:
                        splitd = line.split()
                        if splitd[0] == "property":
                            start_count = True
                        if start_count:
                            if counter == 1:
                                dict_of_vals["min_hom"] = float(splitd[2].strip("%"))
                                dict_of_vals["max_hom"] = float(splitd[3].strip("%"))
                            elif counter == 2:
                                dict_of_vals["min_het"] = float(splitd[2].strip("%"))
                                dict_of_vals["max_het"] = float(splitd[3].strip("%"))
                            elif counter == 3:
                                dict_of_vals["min_hap_len"] =  splitd[3].replace("," , "")
                                dict_of_vals["max_hap_len"] =  splitd[5].replace("," , "")
                            elif counter == 4:
                                dict_of_vals["min_rep_len"] =  splitd[3].replace("," , "")
                                dict_of_vals["max_rep_len"] =  splitd[5].replace("," , "")
                            elif counter == 5:
                                dict_of_vals["min_uniq_len"] = splitd[3].replace("," , "")
                                dict_of_vals["max_uniq_len"] = splitd[5].replace("," , "")
                            elif counter == 6:
                                dict_of_vals["min_model_fit"] = float(splitd[2].strip("%"))
                                dict_of_vals["max_model_fit"] = float(splitd[3].strip("%"))
                            elif counter == 7:
                                dict_of_vals["min_read_error_rate"] = float(splitd[3].strip("%"))
                                dict_of_vals["max_read_error_rate"] = float(splitd[4].strip("%"))
                            counter += 1
                list_of_results.append(dict_of_vals)
        import pandas as pd
        df = pd.DataFrame(list_of_results)
        df.to_csv(output.summary_table, sep="\t", index=False)

rule make_genome_estimate_plot:
    """
    Make a python plot with the genome size estimates (min and max). Should be colored between the min and max
    """
    input:
        summary_table  = config["tool"] + "/summary_tables/{sample}.{k}.summary_table.tsv"
    output:
        genomesize_pdf = config["tool"] + "/genomesize_plots/{sample}.{k}.genomesize.pdf"
    threads: 1
    run:
        # import the necessary libraries to make a plot with matplotlib
        import matplotlib
        matplotlib.use('Agg')
        import matplotlib.pyplot as plt
        #read the summary table into pandas
        df = pd.read_csv(input.summary_table, sep="\t")

        #make a plot
        # the x-axis is the fraction of the total data
        x = df["fraction_of_total_data"]
        y1 = df["min_hap_len"]/1000000
        y2 = df["max_hap_len"]/1000000

        plt.plot(x, y1)
        plt.plot(x, y2)
        
        # Fill the area between the two lines
        plt.fill_between(x, y1, y2)

        # set the y-axis label as "Mb"
        plt.ylabel("Mb")

        # set the x-axis label as "fraction of total data"
        plt.xlabel("fraction of total data")

        # set the title as the sample and k
        plt.title("{sample}, k-{k} genome size estimate".format(sample = wildcards.sample, k = wildcards.k))
        
        # Save the plot as a pdf file
        plt.savefig(output.genomesize_pdf)




#
##DEPRECATED CODE
## This rule was very slow.
##rule randomize_and_split_all_fastqs_per_sample:
##    input:
##        fastqs = lambda wildcards: config["sample"][wildcards.sample]["reads"]
##    output:
##        fastqs = expand(config["tool"] + "/fastqs/{{sample}}/{{sample}}.{num}.fastq.gz",
##                        num=list(range(1,101)))
##    threads: 1
##    run:
##        #open 100 fastq.gz files to write later
##        #write 1/100th of the lines to it
##        output_fastq_gzs = [gzip.open(fqgz, 'wt') for fqgz in output.fastqs]
##
##        # now iterate through the input fastq.gz files and print out lines in succession to the 100 output files.
##        # Use seqio to do the parsing
##        i = 0
##        for fastq in input.fastqs:
##            with gzip.open(fastq, 'rt') as handle:
##                for record in SeqIO.parse(handle, "fastq"):
##                    SeqIO.write(record, output_fastq_gzs[i], "fastq")
##                    i+=1
##                    if i >= len(output_fastq_gzs):
##                        i = 0
##
##        # now close all the output fastq.gz files
##        for fqgz in output_fastq_gzs:
##            fqgz.close()
